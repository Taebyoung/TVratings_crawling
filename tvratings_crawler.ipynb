{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling TVRatings Webpage using Scrapy in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creat a scrapy project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'tvratings_crawler', using template directory '/Users/choetaebyeong/anaconda3/lib/python3.7/site-packages/scrapy/templates/project', created in:\r\n",
      "    /Users/choetaebyeong/Documents/dev/TVratings_crawling/tvratings_crawler\r\n",
      "\r\n",
      "You can start your first spider with:\r\n",
      "    cd tvratings_crawler\r\n",
      "    scrapy genspider example example.com\r\n"
     ]
    }
   ],
   "source": [
    "!rm -rf tvratings_crawler\n",
    "!scrapy startproject tvratings_crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtvratings_crawler\u001b[00m\r\n",
      "├── scrapy.cfg\r\n",
      "└── \u001b[01;34mtvratings_crawler\u001b[00m\r\n",
      "    ├── __init__.py\r\n",
      "    ├── \u001b[01;34m__pycache__\u001b[00m\r\n",
      "    ├── items.py\r\n",
      "    ├── middlewares.py\r\n",
      "    ├── pipelines.py\r\n",
      "    ├── settings.py\r\n",
      "    └── \u001b[01;34mspiders\u001b[00m\r\n",
      "        ├── __init__.py\r\n",
      "        └── \u001b[01;34m__pycache__\u001b[00m\r\n",
      "\r\n",
      "4 directories, 7 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree tvratings_crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Declare items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -*- coding: utf-8 -*-\r\n",
      "\r\n",
      "# Define here the models for your scraped items\r\n",
      "#\r\n",
      "# See documentation in:\r\n",
      "# https://docs.scrapy.org/en/latest/topics/items.html\r\n",
      "\r\n",
      "import scrapy\r\n",
      "\r\n",
      "\r\n",
      "class TvratingsCrawlerItem(scrapy.Item):\r\n",
      "    # define the fields for your item here like:\r\n",
      "    # name = scrapy.Field()\r\n",
      "    pass\r\n"
     ]
    }
   ],
   "source": [
    "!cat tvratings_crawler/tvratings_crawler/items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tvratings_crawler/tvratings_crawler/items.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tvratings_crawler/tvratings_crawler/items.py\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class TvratingsCrawlerItem(scrapy.Item):\n",
    "    genre = scrapy.Field()      # Genre: types of TV programs (General, Drama, Entertainment)\n",
    "    rank = scrapy.Field()       # rank: top 20 rank\n",
    "    program = scrapy.Field()    # program: name of TV program\n",
    "    channel = scrapy.Field()    # channel: name of broadcast channel\n",
    "    rate = scrapy.Field()       # rate: TV Ratings\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write Spider.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tvratings_crawler/tvratings_crawler/spiders/spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tvratings_crawler/tvratings_crawler/spiders/spider.py\n",
    "import scrapy\n",
    "from tvratings_crawler.items import TvratingsCrawlerItem\n",
    "from datetime import date, timedelta\n",
    "\n",
    "class Spider(scrapy.Spider):\n",
    "\n",
    "    name = \"TVratings_Crawler\"\n",
    "    allow_domain = [\"https://search.naver.com/\"]\n",
    "    \n",
    "    def __init__(self, month=12, day=1, **kwargs):\n",
    "        self.month=month        # Use when specifying a date\n",
    "        self.day=day            # Use when specifying a date\n",
    "        today = date.today()\n",
    "        yesterday = date.today() - timedelta(1)\n",
    "        self.month=yesterday.strftime('%m')\n",
    "        self.day=yesterday.strftime('%d')  # If no date is specified, it is used as yesterday's date\n",
    "        super().__init__(**kwargs)\n",
    "  \n",
    "    def start_requests(self, **kwargs):\n",
    "        month = self.month\n",
    "        day = self.day.zfill(2)\n",
    "        urls = [\n",
    "        \"https://search.naver.com/search.naver?where=nexearch&sm=tab_etc&mra=blUw&query={}%EC%9B%94{}%EC%9D%BC%20%EC%A2%85%ED%95%A9%20%EC%8B%9C%EC%B2%AD%EB%A5%A0\".format(month, day),\n",
    "        \"https://search.naver.com/search.naver?where=nexearch&sm=tab_etc&mra=blUw&query={}%EC%9B%94{}%EC%9D%BC%20%EB%93%9C%EB%9D%BC%EB%A7%88%20%EC%8B%9C%EC%B2%AD%EB%A5%A0\".format(month, day),\n",
    "        \"https://search.naver.com/search.naver?where=nexearch&sm=tab_etc&mra=blUw&query={}%EC%9B%94{}%EC%9D%BC%20%EC%98%88%EB%8A%A5%20%EC%8B%9C%EC%B2%AD%EB%A5%A0\".format(month, day),\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for i in range(1, 21):\n",
    "            genre = response.xpath('//*[@id=\"main_pack\"]/div[1]/div/div[2]/div/h4/text()').extract()\n",
    "            rank = response.xpath('//*[@id=\"main_pack\"]/div[1]/div/div[2]/div/div[3]/div/table/tbody/tr[{}]/td[1]/p/span/span/text()'.format(i)).extract()\n",
    "            program = response.xpath('//*[@id=\"main_pack\"]/div[1]/div/div[2]/div/div[3]/div/table/tbody/tr[{}]/td[2]/p/a/text()'.format(i)).extract()\n",
    "            channel = response.xpath('//*[@id=\"main_pack\"]/div[1]/div/div[2]/div/div[3]/div/table/tbody/tr[{}]/td[3]/p/a/text()'.format(i)).extract()\n",
    "            rate = response.xpath('//*[@id=\"main_pack\"]/div[1]/div/div[2]/div/div[3]/div/table/tbody/tr[{}]/td[4]/p/text()'.format(i)).extract()\n",
    "            \n",
    "            item = TvratingsCrawlerItem()\n",
    "            item[\"genre\"] = genre\n",
    "            item[\"rank\"] = rank\n",
    "            item[\"program\"] = program\n",
    "            item[\"channel\"] = channel\n",
    "            item[\"rate\"] = rate\n",
    "            yield item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Change  settings.py in case forbidden by robots.txt \n",
    "    - Go to settings.py in the project folder and change ROBOTSTXT_OBEY = True \n",
    "      to ROBOTSTXT_OBEY = False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBOTSTXT_OBEY = True\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"ROBOTSTXT_OBEY\" tvratings_crawler/tvratings_crawler/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i.bak 's/ROBOTSTXT_OBEY = True/ROBOTSTXT_OBEY = False/' tvratings_crawler/tvratings_crawler/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBOTSTXT_OBEY = False\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"ROBOTSTXT_OBEY\" tvratings_crawler/tvratings_crawler/settings.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Install pymongo for saving the crawling results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pymongo                            3.9.0     \r\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /home/ubuntu/.pyenv/versions/3.6.5/envs/python3/lib/python3.6/site-packages (3.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pymongo in /home/ubuntu/.pyenv/versions/3.6.5/envs/python3/lib/python3.6/site-packages (3.10.0)\r\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  Create mongodb module  to connect to the MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tvratings_crawler/tvratings_crawler/mongodb.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tvratings_crawler/tvratings_crawler/mongodb.py\n",
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient('mongodb://xxx.xxx.xxx.xxx:27017/')  # MongoDB IP Address \n",
    "db = client.tvrating_crawler\n",
    "collection = db.tvrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['15.164.198.126:27017'], document_class=dict, tz_aware=False, connect=True), 'tvrating_crawler'), 'tvrating')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tvratings_crawler.tvratings_crawler.mongodb import collection\n",
    "collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.  Write Pipelines.py to send items to the mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tvratings_crawler/tvratings_crawler/pipelines.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tvratings_crawler/tvratings_crawler/pipelines.py\n",
    "from .mongodb import collection\n",
    "\n",
    "class TVratings_Pipeline(object):\n",
    "    \n",
    "    def process_item(self, item, spider):\n",
    "        \n",
    "        data = {\"genre\": item[\"genre\"], \n",
    "                \"rank\": item[\"rank\"],  \n",
    "                \"channel\": item[\"channel\"], \n",
    "                \"program\": item[\"program\"], \n",
    "                \"rate\": item[\"rate\"], \n",
    "               }\n",
    "        \n",
    "        collection.insert(data)\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.  Add the pipeline to the ITEM_PIPELINES setting in settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#HTTPCACHE_IGNORE_HTTP_CODES = []\r\n",
      "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\r\n",
      "ITEM_PIPELINES = {\r\n",
      "   'tvratings_crawler.pipelines.TVratings_Pipeline': 300,\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"ITEM_PIPELINES = {\"  >> tvratings_crawler/tvratings_crawler/settings.py\n",
    "!echo \"   'tvratings_crawler.pipelines.TVratings_Pipeline': 300,\" >> tvratings_crawler/tvratings_crawler/settings.py\n",
    "!echo \"}\" >> tvratings_crawler/tvratings_crawler/settings.py\n",
    "!tail -n 5 tvratings_crawler/tvratings_crawler/settings.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.  Run TVratings_Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_tvratings_crawler.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_tvratings_crawler.sh\n",
    "cd ~/Documents/dev/TVratings_crawling/\n",
    "rm -rf tvratings_crawler/tvratings_crawler.csv\n",
    "cd tvratings_crawler/\n",
    "scrapy crawl tvratings_crawler -o tvratings_crawler.csv \n",
    "\n",
    "# How to crawl by specifying a datev (for example, December 6)\n",
    "# - \"scrapy crawl TVrating_Crawler -o tvrating_crawler.csv -a month=12 -a day=6\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-13 12:48:14 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: tvratings_crawler)\n",
      "2019-12-13 12:48:14 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.3 (default, Mar 27 2019, 16:54:48) - [Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.7, Platform Darwin-16.7.0-x86_64-i386-64bit\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/choetaebyeong/anaconda3/lib/python3.7/site-packages/scrapy/spiderloader.py\", line 69, in load\n",
      "    return self._spiders[spider_name]\n",
      "KeyError: 'tvratings_crawler'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/choetaebyeong/anaconda3/bin/scrapy\", line 10, in <module>\n",
      "    sys.exit(execute())\n",
      "  File \"/Users/choetaebyeong/anaconda3/lib/python3.7/site-packages/scrapy/cmdline.py\", line 146, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"/Users/choetaebyeong/anaconda3/lib/python3.7/site-packages/scrapy/cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"/Users/choetaebyeong/anaconda3/lib/python3.7/site-packages/scrapy/cmdline.py\", line 154, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"/Users/choetaebyeong/anaconda3/lib/python3.7/site-packages/scrapy/commands/crawl.py\", line 57, in run\n",
      "    self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"/Users/choetaebyeong/anaconda3/lib/python3.7/site-packages/scrapy/crawler.py\", line 183, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"/Users/choetaebyeong/anaconda3/lib/python3.7/site-packages/scrapy/crawler.py\", line 216, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"/Users/choetaebyeong/anaconda3/lib/python3.7/site-packages/scrapy/crawler.py\", line 220, in _create_crawler\n",
      "    spidercls = self.spider_loader.load(spidercls)\n",
      "  File \"/Users/choetaebyeong/anaconda3/lib/python3.7/site-packages/scrapy/spiderloader.py\", line 71, in load\n",
      "    raise KeyError(\"Spider not found: {}\".format(spider_name))\n",
      "KeyError: 'Spider not found: tvratings_crawler'\n"
     ]
    }
   ],
   "source": [
    "!chmod +x run_tvratings_crawler.sh\n",
    "!./run_tvratings_crawler.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: tvratings_crawler/tvratings_crawler.csv: No such file or directory\n",
      "cat: tvratings_crawler/tvratings_crawler.csv: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -altr tvratings_crawler/tvratings_crawler.csv \n",
    "!cat tvratings_crawler/tvratings_crawler.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.  Change the format of result using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre,rank,channel,program,rate\r\n",
      "종합 일간 시청률,1,KBS1,꽃길만걸어요,17.2%\r\n",
      "종합 일간 시청률,2,SBS,VIP2부,13.2%\r\n",
      "종합 일간 시청률,3,KBS2,우아한모녀,13.0%\r\n",
      "종합 일간 시청률,4,KBS1,KBS뉴스9,10.9%\r\n",
      "종합 일간 시청률,5,KBS1,인간극장,10.2%\r\n",
      "종합 일간 시청률,6,SBS,VIP1부,10.0%\r\n",
      "종합 일간 시청률,7,KBS1,아침마당,9.8%\r\n",
      "종합 일간 시청률,8,KBS1,6시내고향,8.3%\r\n",
      "종합 일간 시청률,9,KBS1,KBS뉴스7,7.7%\r\n",
      "종합 일간 시청률,10,KBS1,이웃집찰스,7.3%\r\n",
      "종합 일간 시청률,11,SBS,불타는청춘1부,6.7%\r\n",
      "종합 일간 시청률,11,SBS,본격연예한밤2부,6.7%\r\n",
      "종합 일간 시청률,11,SBS,불타는청춘2부,6.7%\r\n",
      "종합 일간 시청률,14,SBS,본격연예한밤1부,6.4%\r\n",
      "종합 일간 시청률,15,KBS1,KBS930뉴스,6.1%\r\n",
      "종합 일간 시청률,15,KBS1,KBS뉴스광장2부,6.1%\r\n",
      "종합 일간 시청률,17,KBS2,2TV생생정보,5.9%\r\n",
      "종합 일간 시청률,17,SBS,맛좀보실래요,5.9%\r\n",
      "종합 일간 시청률,19,MBC,MBC뉴스데스크,4.5%\r\n",
      "종합 일간 시청률,20,KBS1,동물의왕국,4.4%\r\n",
      "예능 일간 시청률,1,KBS1,아침마당,9.8%\r\n",
      "예능 일간 시청률,2,KBS1,이웃집찰스,7.3%\r\n",
      "예능 일간 시청률,3,SBS,본격연예한밤2부,6.7%\r\n",
      "예능 일간 시청률,3,SBS,불타는청춘1부,6.7%\r\n",
      "예능 일간 시청률,3,SBS,불타는청춘2부,6.7%\r\n",
      "예능 일간 시청률,6,SBS,본격연예한밤1부,6.4%\r\n",
      "예능 일간 시청률,7,KBS1,역사저널그날,3.4%\r\n",
      "예능 일간 시청률,8,KBS1,더라이브,2.9%\r\n",
      "예능 일간 시청률,8,KBS2,정해인의걸어보고서2부,2.9%\r\n",
      "예능 일간 시청률,10,MBC,편애중계2부,2.5%\r\n",
      "예능 일간 시청률,11,MBC,편애중계1부,2.2%\r\n",
      "예능 일간 시청률,12,SBS,좋은아침,2.1%\r\n",
      "예능 일간 시청률,13,SBS,영재발굴단,1.9%\r\n",
      "예능 일간 시청률,14,KBS2,정해인의걸어보고서1부,1.7%\r\n",
      "예능 일간 시청률,15,SBS,TV동물농장,1.5%\r\n",
      "예능 일간 시청률,15,KBS2,슬기로운어른이생활2부,1.5%\r\n",
      "예능 일간 시청률,17,KBS2,옥탑방의문제아들,1.4%\r\n",
      "예능 일간 시청률,17,KBS2,슬기로운어른이생활1부,1.4%\r\n",
      "예능 일간 시청률,19,KBS1,가요무대,1.3%\r\n",
      "예능 일간 시청률,20,MBC,기분좋은날,1.2%\r\n",
      "드라마 일간 시청률,1,KBS1,꽃길만걸어요,17.2%\r\n",
      "드라마 일간 시청률,2,SBS,VIP2부,13.2%\r\n",
      "드라마 일간 시청률,3,KBS2,우아한모녀,13.0%\r\n",
      "드라마 일간 시청률,4,SBS,VIP1부,10.0%\r\n",
      "드라마 일간 시청률,5,SBS,맛좀보실래요,5.9%\r\n",
      "드라마 일간 시청률,6,MBC,나쁜사랑,3.1%\r\n",
      "드라마 일간 시청률,7,KBS2,꽃길만걸어요,2.6%\r\n",
      "드라마 일간 시청률,8,MBC,나쁜사랑,0.4%\r\n",
      "드라마 일간 시청률,0,-,nan,-\r\n",
      "드라마 일간 시청률,0,-,nan,-\r\n",
      "드라마 일간 시청률,0,-,nan,-\r\n",
      "드라마 일간 시청률,0,-,nan,-\r\n",
      "드라마 일간 시청률,0,-,nan,-\r\n",
      "드라마 일간 시청률,0,-,nan,-\r\n",
      "드라마 일간 시청률,0,-,nan,-\r\n",
      "드라마 일간 시청률,0,-,nan,-\r\n",
      "드라마 일간 시청률,0,-,nan,-\r\n",
      "드라마 일간 시청률,0,-,nan,-\r\n",
      "드라마 일간 시청률,0,-,nan,-\r\n",
      "드라마 일간 시청률,0,-,nan,-\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"tvratings_crawler/tvratings_crawler.csv\", na_values = ['N/A', 'NA', 'nan', 'NaN', 'null'])      \n",
    "df_sort = df[['genre','rank','channel','program','rate']].sort_values(by=['genre','rank'], ascending=[False, True])\n",
    "\n",
    "df_sort['rank'] = [x.replace('nan','0') for x in df_sort['rank'].astype(str)]\n",
    "df_sort['rank'] = df_sort['rank'].apply(pd.to_numeric).astype({'rank': int})\n",
    "df_sort['program'] = [x.replace(' ','') for x in df_sort['program'].astype(str)]\n",
    "\n",
    "!rm -rf tvratings_crawler/tvratings_crawler_sort.csv\n",
    "df_sort.to_csv(\"tvratings_crawler/tvratings_crawler_sort.csv\", index=False, na_rep='-')\n",
    "df_sort.to_csv(\"tvratings_crawler/tvratings_crawler_mail.csv\", encoding='cp949', index=False, na_rep='-')\n",
    "\n",
    "!cat tvratings_crawler/tvratings_crawler_sort.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.  Sending email with attachments after crawling the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email import encoders\n",
    "from email.mime.multipart import MIMEMultipart \n",
    "from email.mime.text import MIMEText           \n",
    "from email.mime.base import MIMEBase           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib, os, pickle\n",
    "\n",
    "mail_addr = \"from_email@gmail.com\"                      # email account\n",
    "pw = \"PASSWORD\"                                         # email account password\n",
    "to_addr = [\"to_email@naver.com\",\"to_email@naver.com\" ]  # recipient's email address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smtp = smtplib.SMTP(\"smtp.gmail.com\", 587)\n",
    "smtp.ehlo()\n",
    "smtp.starttls()\n",
    "smtp.login(mail_addr, pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# email subject\n",
    "msg = MIMEMultipart()\n",
    "msg[\"subject\"] = \"일일시청률_순위를_보내드립니다.\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# email body\n",
    "part1 = MIMEText(\"로봇이 필요한 분께 자동으로 전송하여 보내드립니다.\") \n",
    "msg.attach(part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# email body - link url\n",
    "part2 = MIMEText(\"<a href='https://search.naver.com/search.naver?sm=top_hty&fbm=1&ie=utf8&query=%EC%8B%9C%EC%B2%AD%EB%A5%A0%EC%88%9C%EC%9C%84'> 일일 시청률 클릭!!!</a>\", \"html\")\n",
    "msg.attach(part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy attachment file\n",
    "import shutil\n",
    "\n",
    "path = \"./tvrating_crawler/tvrating_crawler_sort.csv\"\n",
    "shutil.copy2(path, 'tvrating_crawler_sort.csv')\n",
    "!ls -ltr tvrating_crawler_sort.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy and attache the result file\n",
    "import shutil\n",
    "\n",
    "file = \"./tvratings_crawler/tvratings_crawler_sort.csv\"\n",
    "shutil.copy2(file, 'tvratings_crawler_mail.csv')\n",
    "!ls -ltr tvratings_crawler_mail.csv\n",
    "\n",
    "path = \"tvratings_crawler_mail.csv\"\n",
    "with open(path, \"rb\") as f:\n",
    "    part3 = MIMEBase(\"application\", \"octet-stream\")\n",
    "    part3.set_payload(f.read())\n",
    "    encoders.encode_base64(part3)\n",
    "    \n",
    "part3.add_header(\"Content-Disposition\", \"attachment\", filename=path)\n",
    "msg.attach(part3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send mail\n",
    "for addr in to_addr:\n",
    "    print(addr)\n",
    "    msg[\"to\"] = addr\n",
    "    smtp.sendmail(mail_addr, addr, msg.as_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
